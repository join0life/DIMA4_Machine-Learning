{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83bd3bf-4b30-4dbf-90b8-d7d7b41e58f7",
   "metadata": {},
   "source": [
    "# 텍스트 사전 준비 작업 - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f70022f-3735-4fd6-8fb3-e8a383dac2a6",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84404edc-2986-417f-818b-5ad811bb619c",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것\n",
    "- nltk : https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6bef364-51fa-4621-b5e3-ba739e39c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0b4447-cae9-4944-8d65-4ab7ab5a7fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentence = sent_tokenize(text_sample)\n",
    "print(len(sentence))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f319512-5ec8-4693-8b4a-0f86a60dfa1e",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화 하는 것\n",
    "- 일반적으로 문장 토큰화는 각 문장이 가지는 의미가 중요한 요소로 사용될 때 사용\n",
    "- BoW(Bag of Word)와 같이 단어의 순서가 중요하지 않는 경우 단어 토큰화만 해도 충분하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ada0e72-3e1e-48f3-a808-b5c31bb28b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd805d58-5cce-4b9a-be7d-164ae976c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [ word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae040dc-2829-40d1-8698-11b4b2ec3d43",
   "metadata": {},
   "source": [
    "## Stopwords 제거\n",
    "- 분석에 큰 의미가 없는 단어를 지칭한다.\n",
    "- is, the, a, will 등 문맥적으로 큰 의미가 없는 단어가 이에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09b3f6-f7b9-4698-91f7-29661214604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # 불용어 목록을 다운 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a4ec30-abde-4fc0-b71b-cb130d037551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6988cbf6-32cd-4f4a-aa53-2053fc0bf3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 불용어 개수:  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 불용어 개수: ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0575e6-3dde-45a5-a1ed-d5a01f4a2a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# 구두점 목록\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6488f0cc-486f-4dfa-92a0-f0ef6e5ab6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[원본 단어]\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "\n",
      "[불용어 제거 단어]\n",
      "[['matrix', 'everywhere', 'around', 'us', 'even', 'room'], ['see', 'window', 'television'], ['feel', 'go', 'work', 'go', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "print('[원본 단어]')\n",
    "print(word_tokens)\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in string.punctuation:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "print()\n",
    "print('[불용어 제거 단어]')\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1fe4e-d0e0-45cf-8c84-fb5cd1189d99",
   "metadata": {},
   "source": [
    "## 어간추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "- 문법적 또는 의미적으로 변화하는 `단어의 원형을 찾는다.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1831409-7fec-4d4e-8f0e-5bec1062f00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amused'), stemmer.stem('amuses'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f201e26-deed-4122-89ab-4b3e93e20fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('wordnet') # 사전 다운로드\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "# 명사(n), 동사(v), 형용사(a), 부사(r)\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54813404-9261-45e4-a476-226fcf8927c0",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11092adc-74c3-4dcf-a6b1-ab0e3c90a335",
   "metadata": {},
   "source": [
    "## DTM(Document Term Matrix, 문서 단어 행렬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3407e4-5c7d-492b-9641-8055d1b4154d",
   "metadata": {},
   "source": [
    "### CountVectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91fd8123-67f3-4332-a1fb-f8d77e6a7664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 2 1 2 1]]\n",
      "vocabulary: [('because', 0), ('know', 1), ('love', 2), ('want', 3), ('you', 4), ('your', 5)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print('bag of words vector:', vector.fit_transform(corpus).toarray())\n",
    "# 'I'는 BoW를 만드는 과정에서 제외됨(CountVectorizer는 기본적으로 길이가 2 이상인 단어만 토큰으로 인식)\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key= lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d75f3-0c71-4460-87cf-0b60c4c5e4de",
   "metadata": {},
   "source": [
    "- 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830f9eb-46f0-4e4f-8f77-0ce032126b47",
   "metadata": {},
   "source": [
    "1. 사용자 정의 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6348217d-14bb-4bae-8da0-f1f331dc5e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "vocabulary: [('everything', 0), ('family', 1), ('important', 2), ('it', 3), ('thing', 4)]\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vector = CountVectorizer(stop_words=['the','an','a','is','not'])\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key= lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd18d4-25a2-42b3-bb99-fb9a3ffa4dfd",
   "metadata": {},
   "source": [
    "2. CountVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0d004e6-920e-4736-8001-6245dfac99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "vocabulary: [('family', 0), ('important', 1), ('thing', 2)]\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vector = CountVectorizer(stop_words='english')\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key= lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae81340-c1ea-4351-ba1a-de2504e88607",
   "metadata": {},
   "source": [
    "3. NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39e55cbe-c600-48cd-9efd-10b8cabe74bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "vocabulary: [('everything', 0), ('family', 1), ('important', 2), ('thing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words('english')\n",
    "vector = CountVectorizer(stop_words=stop_words)\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key= lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb0c77-8417-433c-b9b9-93ec2a2eef7d",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "131e9135-de27-4add-bf12-f68f1e62d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "vocabulary: [('do', 0), ('know', 1), ('like', 2), ('love', 3), ('should', 4), ('want', 5), ('what', 6), ('you', 7), ('your', 8)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', # 문서 1\n",
    "         'I like you', # 문서 2\n",
    "         'what should I do'] # 문서 3\n",
    "tfidf = TfidfVectorizer()\n",
    "print(tfidf.fit_transform(corpus).toarray())\n",
    "print('vocabulary:', sorted(tfidf.vocabulary_.items(), key= lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb6499-4951-4e43-8048-46068f8557d8",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c41f2e-eb87-4bdb-a85b-ac817d276ceb",
   "metadata": {},
   "source": [
    "## 패키지 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc425767-5944-4ede-9a1b-7ca4147bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d2bc9-4cc2-43a3-9393-b7c2d7bbe38d",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25040cf4-2896-4ee9-b724-d6f82ca21d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x20803e27190>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c5b670-b7c5-470f-a710-5791189deac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ted_en-20160408.xml\", 'r', encoding='UTF8') as f:\n",
    "    ted_xml = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57d0888-bd9f-48e8-89d3-79dd72ca4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<xml language=\"en\"><file id=\"1\">\n",
      "  <head>\n",
      "    <url>http://www.ted.com/talks/knut_haanaes_two_reasons_companies_fail_and_how_to_avoid_them</url>\n",
      "    <pagesize>72832</pagesize>\n",
      "    <dtime>Fri Apr 01 00:57:03 CEST 2016</dtime>\n",
      "    <encoding>UTF-8</encoding>\n",
      "    <content-type>text/html; charset=utf-8</content-type>\n",
      "    <keywords>talks, business, creativity, curiosity, goal-setting, innovation, motivation, potential, success, work</keywords>\n",
      "    <speaker>Knut Haanaes</speaker>\n",
      "    <talkid>2470</talkid>\n",
      "    <videourl>http://download.ted.com/talks/KnutHaanaes_2015S.mp4</videourl>\n",
      "    <videopath>talks/KnutHaanaes_2015S.mp4</videopath>\n",
      "    <date>2015/06/30</date>\n",
      "    <title>Knut Haanaes: Two reasons companies fail -- and how to avoid them</title>\n",
      "    <description>TED Talk Subtitles and Transcript: Is it possible to run a company and reinvent it at the same time? For business strategist Knut Haanaes, the ability to innovate after becoming successful is the \n"
     ]
    }
   ],
   "source": [
    "print(ted_xml[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b772452-dfa0-491a-b1a4-1ef1e954fed4",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "- 학습할 문서는 <content>~</content> 안에 있으며, 이 내용만 가져온다.\n",
    "- 문서 안에 포함된 학습에 불필요한 문장들 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db00b632-e3c6-423c-b6da-e525f55587ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(ted_xml, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3366ada0-6336-4a44-bff3-71bf9365a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\n",
      "Consider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. And what did Facit do when the electronic calculator came along? They continued doing exactly the same. In six months, they went from maximum revenue ... and they were gone. Gone.\n",
      "To me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators.\n",
      "\n",
      "Facit did too much exploitation. But exploration can go wild, too.\n",
      "A few years back, I worked closely alongside a European biotech co\n"
     ]
    }
   ],
   "source": [
    "content_text = bs.find_all('content')\n",
    "# print(len(content_text))\n",
    "# print(content_text[0])\n",
    "content_text = '\\n'.join( c.get_text() for c in content_text)\n",
    "content_text = re.sub(r'\\([\\w]*\\)','',content_text)\n",
    "# print(content_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796615ab-c535-4e8f-bb47-224d86756705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(content_text)\n",
    "\n",
    "# 구두점 제거 및 소문자 변환\n",
    "nomalized_text = []\n",
    "for sentence in sentences:\n",
    "    result = re.sub('[^\\w]+',' ',sentence.lower())\n",
    "    nomalized_text.append(result)\n",
    "\n",
    "# 단어 토큰화\n",
    "result = [word_tokenize(sentence) for sentence in nomalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d9e9b55-9c53-47c4-bbfc-1105f90f902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 개수 :273,698\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n"
     ]
    }
   ],
   "source": [
    "print(f'전체 샘플 개수 :{len(result):,}')\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e4ffd-6d63-4257-ba0e-b0f256d0cc74",
   "metadata": {},
   "source": [
    "## Word2Vec 학습\n",
    "- Word2Vec 주요 파라메터\n",
    ">- vector_size : 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원 (적절한 임베딩 벡터의 차원의 크기는 100~300 사이의 값 지정)\n",
    ">- window : 컨텍스트 윈도우 크기\n",
    ">- min_count : 단어 최소 빈도 수 제한(빈도가 적은 단어들은 학습에서 제외)\n",
    ">- sg : 0은 CBOW, 1은 Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb0a916-6faa-492e-adec-8b01f5d83fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19084e1a-8cb2-4acd-bd30-9a89a8b1c98d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8412037491798401), ('guy', 0.8095774054527283), ('boy', 0.763991117477417), ('lady', 0.7517896294593811), ('girl', 0.7424473762512207), ('soldier', 0.7386289238929749), ('gentleman', 0.7186206579208374), ('kid', 0.6830040812492371), ('poet', 0.6681225895881653), ('photographer', 0.6545884013175964)]\n"
     ]
    }
   ],
   "source": [
    "sim_word = model.wv.most_similar('man')\n",
    "print(sim_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c0224-78cf-489b-96ad-85cba91c370f",
   "metadata": {},
   "source": [
    "## Word2Vec 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5951fee3-ebde-4189-82a6-76e870c54d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "\n",
    "# 모델 로드\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f6a37-a66a-4a0b-83c5-9eaf2a851e7b",
   "metadata": {},
   "source": [
    "## 임베딩 벡터의 시각화\n",
    "- 구글의 임베딩 프로젝터라는 시각화 도구를 통해 학습한 임베딩 벡터를 시각화할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02131f3-f0ec-47cf-b2ea-43d1ff6aec2c",
   "metadata": {},
   "source": [
    "### 워드 임베딩 모델로부터 tsv 파일 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f77172ec-94d6-4901-9c9d-a018c9421063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 11:38:42,867 - word2vec2tensor - INFO - running C:\\Users\\user\\anaconda3\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input eng_w2v --output eng_w2v\n",
      "2024-05-02 11:38:42,867 - keyedvectors - INFO - loading projection weights from eng_w2v\n",
      "2024-05-02 11:38:44,615 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21638, 100) matrix of type float32 from eng_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-05-02T11:38:44.569316', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'load_word2vec_format'}\n",
      "2024-05-02 11:38:45,998 - word2vec2tensor - INFO - 2D tensor file saved to eng_w2v_tensor.tsv\n",
      "2024-05-02 11:38:45,998 - word2vec2tensor - INFO - Tensor metadata file saved to eng_w2v_metadata.tsv\n",
      "2024-05-02 11:38:45,998 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349f805-5f2c-4ce7-b8c9-9761b717ecf7",
   "metadata": {},
   "source": [
    "### 임베딩 프로젝터를 이용한 시각화\n",
    "- https://projector.tensorflow.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
